# requirements.txt
# ITS Playground - Python Dependencies

# Core Data Processing
pandas>=2.0.0
numpy>=1.24.0
scipy>=1.10.0

# Machine Learning
scikit-learn>=1.3.0
tensorflow>=2.13.0
xgboost>=1.7.0
lightgbm>=4.0.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.15.0
bokeh>=3.2.0

# Computer Vision (for RoadAid AI)
opencv-python>=4.8.0
Pillow>=10.0.0
imageio>=2.31.0

# Web Applications
streamlit>=1.25.0
flask>=2.3.0
fastapi>=0.100.0
uvicorn>=0.23.0

# Time Series Analysis
prophet>=1.1.0
statsmodels>=0.14.0

# Jupyter and Notebooks
jupyter>=1.0.0
jupyterlab>=4.0.0
ipykernel>=6.25.0
ipywidgets>=8.1.0

# Database and Storage
sqlite3
sqlalchemy>=2.0.0
psycopg2-binary>=2.9.0

# API and HTTP
requests>=2.31.0
aiohttp>=3.8.0

# Utilities
tqdm>=4.65.0
python-dateutil>=2.8.0
pytz>=2023.3
pyyaml>=6.0

# Development and Testing
pytest>=7.4.0
black>=23.7.0
flake8>=6.0.0

---

# Dockerfile
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    software-properties-common \
    git \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create data directories
RUN mkdir -p /app/data /app/models /app/results /app/logs

# Expose ports for different services
EXPOSE 8501 8502 8503 5000

# Set environment variables
ENV PYTHONPATH=/app
ENV STREAMLIT_SERVER_PORT=8501
ENV FLASK_PORT=5000

# Default command (can be overridden)
CMD ["streamlit", "run", "Traffic_Analytics/Traffic_Passage_Dashboard/dashboard.py"]

---

# docker-compose.yml
version: '3.8'

services:
  its-analytics:
    build: .
    container_name: its-analytics
    ports:
      - "8501:8501"  # Streamlit dashboard
      - "8888:8888"  # Jupyter notebooks
      - "5000:5000"  # Flask API
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./results:/app/results
      - ./notebooks:/app/notebooks
    environment:
      - PYTHONPATH=/app
      - JUPYTER_ENABLE_LAB=yes
    command: jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --LabApp.token='' --LabApp.password=''

  its-roadaid:
    build: .
    container_name: its-roadaid
    ports:
      - "8502:8502"
    volumes:
      - ./Road_Safety/RoadAid_AI:/app/roadaid
      - ./data:/app/data
    working_dir: /app/roadaid
    command: streamlit run --server.port=8502 app/roadaid_dashboard.py

  its-ev-optimizer:
    build: .
    container_name: its-ev-optimizer
    ports:
      - "8503:8503"
    volumes:
      - ./EV_Charging:/app/ev_charging
      - ./data:/app/data
    working_dir: /app/ev_charging
    command: streamlit run --server.port=8503 visualization.py

  its-api:
    build: .
    container_name: its-api
    ports:
      - "5001:5000"
    volumes:
      - ./:/app
    environment:
      - FLASK_APP=api/main.py
      - FLASK_ENV=development
    command: python api/main.py

  postgres:
    image: postgres:15
    container_name: its-postgres
    environment:
      POSTGRES_DB: its_playground
      POSTGRES_USER: its_user
      POSTGRES_PASSWORD: its_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql

  redis:
    image: redis:7-alpine
    container_name: its-redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:

networks:
  default:
    name: its-network

---

# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: its-analytics-deployment
  labels:
    app: its-analytics
spec:
  replicas: 2
  selector:
    matchLabels:
      app: its-analytics
  template:
    metadata:
      labels:
        app: its-analytics
    spec:
      containers:
      - name: its-analytics
        image: its-playground:latest
        ports:
        - containerPort: 8501
        - containerPort: 5000
        env:
        - name: PYTHONPATH
          value: "/app"
        - name: DB_HOST
          value: "postgres-service"
        - name: REDIS_HOST
          value: "redis-service"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        volumeMounts:
        - name: data-volume
          mountPath: /app/data
        - name: models-volume
          mountPath: /app/models
      volumes:
      - name: data-volume
        persistentVolumeClaim:
          claimName: its-data-pvc
      - name: models-volume
        persistentVolumeClaim:
          claimName: its-models-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: its-analytics-service
spec:
  selector:
    app: its-analytics
  ports:
    - name: streamlit
      protocol: TCP
      port: 8501
      targetPort: 8501
    - name: api
      protocol: TCP
      port: 5000
      targetPort: 5000
  type: LoadBalancer

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: its-data-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: its-models-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

---

# ci-cd-pipeline.yml (GitHub Actions)
name: ITS Playground CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_its
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-cov
    
    - name: Generate test data
      run: |
        python synthetic_data_generator.py
    
    - name: Run tests
      run: |
        pytest --cov=./ --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  build:
    needs: test
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Login to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: |
          ${{ secrets.DOCKER_USERNAME }}/its-playground:latest
          ${{ secrets.DOCKER_USERNAME }}/its-playground:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to Kubernetes
      run: |
        echo "Deploying to production cluster..."
        # Add your K8s deployment commands here
        
---

# .github/workflows/data-validation.yml
name: Data Validation

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM
  workflow_dispatch:

jobs:
  validate-data:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install pandas numpy great-expectations
    
    - name: Run data validation
      run: |
        python scripts/data_validation.py
    
    - name: Generate data quality report
      run: |
        python scripts/generate_data_report.py
    
    - name: Upload report
      uses: actions/upload-artifact@v3
      with:
        name: data-quality-report
        path: reports/

---

# scripts/setup.sh
#!/bin/bash

echo "ðŸš¦ ITS Playground Setup Script"
echo "=============================="

# Create directory structure
echo "ðŸ“ Creating directory structure..."
mkdir -p {Traffic_Analytics/{Traffic_Passage_Dashboard/{data,notebooks},Vehicle_Passage_Prediction/{data,notebooks,models}},Road_Safety/RoadAid_AI/{data,notebooks,app,models},Tolling_System/Log_Analysis_Automation/{sample_logs,results},EV_Charging/EV_Integration_Model/{data,notebooks},DevOps_Deployment/kubernetes,api,scripts,tests,reports}

# Generate synthetic data
echo "ðŸ“Š Generating synthetic datasets..."
python synthetic_data_generator.py

# Set up Python environment
echo "ðŸ Setting up Python environment..."
python -m venv its_env
source its_env/bin/activate  # On Windows: its_env\Scripts\activate
pip install -r requirements.txt

# Initialize Jupyter kernel
echo "ðŸ““ Setting up Jupyter kernel..."
python -m ipykernel install --user --name=its_playground --display-name="ITS Playground"

# Create sample configuration files
echo "âš™ï¸ Creating configuration files..."
cat > config/config.yaml << EOL
# ITS Playground Configuration
database:
  host: localhost
  port: 5432
  name: its_playground
  user: its_user
  password: its_password

redis:
  host: localhost
  port: 6379

api:
  host: 0.0.0.0
  port: 5000
  debug: true

ml_models:
  traffic_prediction:
    model_path: models/traffic_predictor.pkl
    retrain_interval: 24  # hours
  
  anomaly_detection:
    threshold: 0.95
    window_size: 100

visualization:
  refresh_interval: 30  # seconds
  cache_timeout: 300   # seconds
EOL

# Initialize database schema
echo "ðŸ—„ï¸ Setting up database schema..."
cat > sql/init.sql << EOL
-- ITS Playground Database Schema

CREATE DATABASE IF NOT EXISTS its_playground;
USE its_playground;

-- Passage logs table
CREATE TABLE passage_logs (
    passage_id VARCHAR(20) PRIMARY KEY,
    vehicle_id VARCHAR(20),
    timestamp TIMESTAMP,
    axle_count INT,
    vehicle_class VARCHAR(20),
    speed_kmh DECIMAL(5,2),
    lane_number INT,
    direction VARCHAR(10),
    tag_id VARCHAR(20),
    tag_detected BOOLEAN,
    gps_latitude DECIMAL(10,6),
    gps_longitude DECIMAL(11,6),
    sensor_id VARCHAR(20),
    image_captured BOOLEAN,
    trip_start BOOLEAN,
    INDEX idx_timestamp (timestamp),
    INDEX idx_vehicle_id (vehicle_id),
    INDEX idx_lane (lane_number)
);

-- Transaction logs table
CREATE TABLE toll_transactions (
    transaction_id VARCHAR(20) PRIMARY KEY,
    passage_id VARCHAR(20),
    vehicle_id VARCHAR(20),
    timestamp TIMESTAMP,
    toll_amount DECIMAL(8,2),
    payment_method VARCHAR(20),
    transaction_status VARCHAR(20),
    processing_time_sec DECIMAL(5,2),
    lane_number INT,
    vehicle_class VARCHAR(20),
    tag_id VARCHAR(20),
    FOREIGN KEY (passage_id) REFERENCES passage_logs(passage_id),
    INDEX idx_timestamp (timestamp),
    INDEX idx_payment_method (payment_method)
);

-- ANPR data table
CREATE TABLE anpr_data (
    anpr_id VARCHAR(20) PRIMARY KEY,
    passage_id VARCHAR(20),
    vehicle_id VARCHAR(20),
    timestamp TIMESTAMP,
    license_plate VARCHAR(20),
    state VARCHAR(5),
    ocr_confidence DECIMAL(5,3),
    image_quality VARCHAR(10),
    weather_condition VARCHAR(20),
    camera_id VARCHAR(20),
    vehicle_make VARCHAR(20),
    vehicle_color VARCHAR(20),
    match_confidence DECIMAL(5,3),
    FOREIGN KEY (passage_id) REFERENCES passage_logs(passage_id),
    INDEX idx_license_plate (license_plate),
    INDEX idx_confidence (ocr_confidence)
);

-- EV charging sessions table
CREATE TABLE ev_charging (
    session_id VARCHAR(20) PRIMARY KEY,
    station_id VARCHAR(20),
    charger_id VARCHAR(20),
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    duration_minutes INT,
    energy_delivered_kwh DECIMAL(8,2),
    power_rating_kw DECIMAL(6,2),
    charging_cost DECIMAL(8,2),
    payment_method VARCHAR(20),
    vehicle_type VARCHAR(20),
    user_id VARCHAR(20),
    station_location VARCHAR(30),
    grid_impact_kw DECIMAL(6,2),
    INDEX idx_start_time (start_time),
    INDEX idx_station (station_id)
);

-- Road safety incidents table
CREATE TABLE road_safety_incidents (
    incident_id VARCHAR(20) PRIMARY KEY,
    timestamp TIMESTAMP,
    incident_type VARCHAR(20),
    severity VARCHAR(10),
    lane_number INT,
    direction VARCHAR(10),
    gps_latitude DECIMAL(10,6),
    gps_longitude DECIMAL(11,6),
    duration_minutes INT,
    vehicles_involved INT,
    injuries INT,
    emergency_response BOOLEAN,
    traffic_impact VARCHAR(20),
    weather_condition VARCHAR(20),
    INDEX idx_timestamp (timestamp),
    INDEX idx_severity (severity)
);

-- Create views for common queries
CREATE VIEW hourly_traffic AS
SELECT 
    DATE(timestamp) as date,
    HOUR(timestamp) as hour,
    COUNT(*) as vehicle_count,
    AVG(speed_kmh) as avg_speed,
    SUM(CASE WHEN tag_detected = 1 THEN 1 ELSE 0 END) as tagged_vehicles
FROM passage_logs
GROUP BY DATE(timestamp), HOUR(timestamp);

CREATE VIEW revenue_summary AS
SELECT 
    DATE(timestamp) as date,
    payment_method,
    COUNT(*) as transaction_count,
    SUM(toll_amount) as total_revenue,
    AVG(toll_amount) as avg_toll,
    AVG(processing_time_sec) as avg_processing_time
FROM toll_transactions
WHERE transaction_status = 'Success'
GROUP BY DATE(timestamp), payment_method;
EOL

# Create test files
echo "ðŸ§ª Setting up tests..."
cat > tests/test_data_generator.py << EOL
import pytest
import pandas as pd
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from synthetic_data_generator import ITSDataGenerator

def test_passage_logs_generation():
    generator = ITSDataGenerator()
    df = generator.generate_passage_logs(num_records=100)
    
    assert len(df) == 100
    assert 'passage_id' in df.columns
    assert 'vehicle_id' in df.columns
    assert 'timestamp' in df.columns
    assert df['speed_kmh'].min() > 0
    assert df['lane_number'].max() <= 8

def test_transaction_data_generation():
    generator = ITSDataGenerator()
    passage_df = generator.generate_passage_logs(num_records=100)
    transaction_df = generator.generate_transaction_data(passage_df)
    
    assert len(transaction_df) > 0
    assert all(col in transaction_df.columns for col in ['transaction_id', 'passage_id', 'toll_amount'])
    assert transaction_df['toll_amount'].min() > 0

def test_data_relationships():
    generator = ITSDataGenerator()
    passage_df = generator.generate_passage_logs(num_records=50)
    transaction_df = generator.generate_transaction_data(passage_df)
    anpr_df = generator.generate_anpr_data(passage_df)
    
    # Test relationships
    passage_ids = set(passage_df['passage_id'])
    transaction_passage_ids = set(transaction_df['passage_id'])
    anpr_passage_ids = set(anpr_df['passage_id'])
    
    assert transaction_passage_ids.issubset(passage_ids)
    assert anpr_passage_ids.issubset(passage_ids)
EOL

# Create API endpoints
echo "ðŸŒ Setting up API..."
cat > api/main.py << EOL
from flask import Flask, jsonify, request
from flask_cors import CORS
import pandas as pd
import json
from datetime import datetime, timedelta

app = Flask(__name__)
CORS(app)

# Load data (in production, use database)
try:
    passage_df = pd.read_csv('passage_logs.csv')
    transaction_df = pd.read_csv('toll_transactions.csv')
    passage_df['timestamp'] = pd.to_datetime(passage_df['timestamp'])
    transaction_df['timestamp'] = pd.to_datetime(transaction_df['timestamp'])
except FileNotFoundError:
    passage_df = pd.DataFrame()
    transaction_df = pd.DataFrame()

@app.route('/api/health')
def health_check():
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'data_loaded': not passage_df.empty
    })

@app.route('/api/traffic/hourly')
def hourly_traffic():
    if passage_df.empty:
        return jsonify({'error': 'No data available'}), 404
    
    hourly_data = passage_df.groupby(passage_df['timestamp'].dt.hour).size()
    return jsonify({
        'hours': hourly_data.index.tolist(),
        'vehicle_counts': hourly_data.values.tolist()
    })

@app.route('/api/revenue/daily')
def daily_revenue():
    if transaction_df.empty:
        return jsonify({'error': 'No data available'}), 404
    
    successful_transactions = transaction_df[transaction_df['transaction_status'] == 'Success']
    daily_revenue = successful_transactions.groupby(successful_transactions['timestamp'].dt.date)['toll_amount'].sum()
    
    return jsonify({
        'dates': [str(date) for date in daily_revenue.index],
        'revenue': daily_revenue.values.tolist()
    })

@app.route('/api/kpis')
def get_kpis():
    if passage_df.empty or transaction_df.empty:
        return jsonify({'error': 'No data available'}), 404
    
    successful_transactions = transaction_df[transaction_df['transaction_status'] == 'Success']
    
    kpis = {
        'total_vehicles': len(passage_df),
        'total_revenue': float(successful_transactions['toll_amount'].sum()),
        'avg_speed': float(passage_df['speed_kmh'].mean()),
        'tag_detection_rate': float(passage_df['tag_detected'].mean()),
        'transaction_success_rate': float((transaction_df['transaction_status'] == 'Success').mean()),
        'last_updated': datetime.now().isoformat()
    }
    
    return jsonify(kpis)

@app.route('/api/anomalies')
def get_anomalies():
    if passage_df.empty or transaction_df.empty:
        return jsonify({'error': 'No data available'}), 404
    
    # Speed violations
    speed_violations = passage_df[passage_df['speed_kmh'] > 100]
    
    # Missing transactions for tagged vehicles
    tagged_passages = passage_df[passage_df['tag_detected'] == True]
    passages_with_transactions = passage_df[passage_df['passage_id'].isin(transaction_df['passage_id'])]
    missing_transactions = tagged_passages[~tagged_passages['passage_id'].isin(passages_with_transactions['passage_id'])]
    
    anomalies = {
        'speed_violations': len(speed_violations),
        'missing_transactions': len(missing_transactions),
        'potential_revenue_loss': len(missing_transactions) * 5.50  # Average toll
    }
    
    return jsonify(anomalies)

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
EOL

# Create monitoring script
echo "ðŸ“Š Setting up monitoring..."
cat > scripts/monitor.py << EOL
import time
import requests
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_api_health():
    try:
        response = requests.get('http://localhost:5000/api/health', timeout=5)
        if response.status_code == 200:
            logger.info(f"API health check passed: {response.json()}")
            return True
        else:
            logger.error(f"API health check failed: {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"API health check error: {e}")
        return False

def monitor_system():
    logger.info("Starting ITS system monitoring...")
    
    while True:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        logger.info(f"[{timestamp}] Running health checks...")
        
        api_healthy = check_api_health()
        
        if not api_healthy:
            logger.error("System health issues detected!")
            # Add alerting logic here
        
        time.sleep(30)  # Check every 30 seconds

if __name__ == '__main__':
    monitor_system()
EOL

# Make scripts executable
chmod +x scripts/setup.sh
chmod +x scripts/monitor.py

echo ""
echo "âœ… ITS Playground setup complete!"
echo ""
echo "ðŸš€ Quick Start Commands:"
echo "  â€¢ Generate data: python synthetic_data_generator.py"
echo "  â€¢ Start Jupyter: jupyter lab"
echo "  â€¢ Run API: python api/main.py"
echo "  â€¢ Run with Docker: docker-compose up"
echo "  â€¢ Run tests: pytest"
echo "  â€¢ Start monitoring: python scripts/monitor.py"
echo ""
echo "ðŸ“– Next Steps:"
echo "  1. Open Traffic_Analytics/notebooks/ for analysis"
echo "  2. Visit http://localhost:8501 for Streamlit dashboard"
echo "  3. Check http://localhost:5000/api/health for API status"
echo "  4. Explore the data in passage_logs.csv"
echo ""
echo "ðŸŽ¯ Happy analyzing!"